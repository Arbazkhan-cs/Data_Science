{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7581d394",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e0465c",
   "metadata": {},
   "source": [
    "## Predicting that a coffee is good or not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a10bded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_shape=(200, 2), y_shape=(200, 1)\n"
     ]
    }
   ],
   "source": [
    "# Training Set\n",
    "def load_coffee_data():\n",
    "    \"\"\" Creates a coffee roasting data set.\n",
    "        roasting duration: 12-15 minutes is best\n",
    "        temperature range: 175-260C is best\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(2)\n",
    "    X = rng.random(400).reshape(-1,2)\n",
    "    X[:,1] = X[:,1] * 4 + 11.5          # 12-15 min is best\n",
    "    X[:,0] = X[:,0] * (285-150) + 150  # 350-500 F (175-260 C) is best\n",
    "    Y = np.zeros(len(X))\n",
    "    \n",
    "    i=0\n",
    "    for t,d in X:\n",
    "        y = -3/(260-175)*t + 21\n",
    "        if (t > 175 and t < 260 and d > 12 and d < 15 and d<=y ):\n",
    "            Y[i] = 1\n",
    "        else:\n",
    "            Y[i] = 0\n",
    "        i += 1\n",
    "\n",
    "    return (X, Y.reshape(-1,1))\n",
    "\n",
    "x_train, y_train = load_coffee_data()\n",
    "\n",
    "print(f\"x_shape={x_train.shape}, y_shape={y_train.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7aca37",
   "metadata": {},
   "source": [
    "## Before implementing the model, first we have to normalise it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b1d8119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature Max, Min pre normalization: 284.99, 151.32\n",
      "Duration    Max, Min pre normalization: 15.45, 11.51\n",
      "Temperature Max, Min post normalization: 1.66, -1.69\n",
      "Duration    Max, Min post normalization: 1.79, -1.70\n"
     ]
    }
   ],
   "source": [
    "# x_train[:, 0] # this will give me the array with first column data only\n",
    "print(f\"Temperature Max, Min pre normalization: {np.max(x_train[:,0]):0.2f}, {np.min(x_train[:,0]):0.2f}\")\n",
    "print(f\"Duration    Max, Min pre normalization: {np.max(x_train[:,1]):0.2f}, {np.min(x_train[:,1]):0.2f}\")\n",
    "normal_l = tf.keras.layers.Normalization()\n",
    "normal_l.adapt(x_train) # learns mean and varience\n",
    "xn_train = normal_l(x_train)\n",
    "print(f\"Temperature Max, Min post normalization: {np.max(xn_train[:,0]):0.2f}, {np.min(xn_train[:,0]):0.2f}\")\n",
    "print(f\"Duration    Max, Min post normalization: {np.max(xn_train[:,1]):0.2f}, {np.min(xn_train[:,1]):0.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b164c11",
   "metadata": {},
   "source": [
    "Tile/copy our data to increase the training set size and reduce the number of training epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e6710b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xt_train.shape = (200000, 2), yt_train.shape = (200000, 1)\n"
     ]
    }
   ],
   "source": [
    "xt_train = np.tile(xn_train, (1000, 1))  # this will create duplecates data 1000 times more \n",
    "yt_train = np.tile(y_train, (1000, 1))\n",
    "\n",
    "print(f\"xt_train.shape = {xt_train.shape}, yt_train.shape = {yt_train.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c39e046",
   "metadata": {},
   "source": [
    "## Tensorflow Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d50283d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    tf.keras.Input(shape=(2,)),\n",
    "    Dense(units=3, activation=\"sigmoid\", name=\"layer1\"),  # hidden layer\n",
    "    Dense(units=1, activation=\"sigmoid\", name=\"layer2\")   # output layer\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96f77051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " layer1 (Dense)              (None, 3)                 9         \n",
      "                                                                 \n",
      " layer2 (Dense)              (None, 1)                 4         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 13 (52.00 Byte)\n",
      "Trainable params: 13 (52.00 Byte)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf799b8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 params =  9 , L2 params =  4\n"
     ]
    }
   ],
   "source": [
    "L1_num_params = 2 * 3 + 3  \n",
    "L2_num_params = 3 * 1 + 1\n",
    "print(\"L1 params = \", L1_num_params, \", L2 params = \", L2_num_params  )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2ce28a",
   "metadata": {},
   "source": [
    "\n",
    "<pre>\n",
    "Let's examine the weights and biases Tensorflow has instantiated. The weights\n",
    "should be of size (number of features in input, number of units in the layer) while the bias\n",
    "size should match the number of units in the layer:\n",
    "\n",
    "     -> In the first layer with 3 units, we expect W to have a size of (2,3) and should have 3 elements.\n",
    "     -> In the second layer with 1 unit, we expect W to have a size of (3,1) andshould have 1 element.\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2be19c2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1(2, 3):\n",
      " [[-0.731013    0.77222764  1.0929687 ]\n",
      " [-1.0322514   0.5378201  -0.5608119 ]] \n",
      "b1(3,): [0. 0. 0.]\n",
      "\n",
      "W2(3, 1):\n",
      " [[-0.9041215 ]\n",
      " [-0.12271202]\n",
      " [ 0.65165055]] \n",
      "b2(1,): [0.]\n"
     ]
    }
   ],
   "source": [
    "w1, b1 = model.get_layer(\"layer1\").get_weights()\n",
    "w2, b2 = model.get_layer(\"layer2\").get_weights()\n",
    "print(f\"W1{w1.shape}:\\n\", w1, f\"\\nb1{b1.shape}:\", b1)\n",
    "print()\n",
    "print(f\"W2{w2.shape}:\\n\", w2, f\"\\nb2{b2.shape}:\", b2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c95d3e2",
   "metadata": {},
   "source": [
    "Above are randomly generated weights and bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1cbc3ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    "    loss = tf.keras.losses.BinaryCrossentropy(),\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea851b49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "6250/6250 [==============================] - 17s 2ms/step - loss: 0.1868 - accuracy: 0.9216\n",
      "Epoch 2/10\n",
      "6250/6250 [==============================] - 15s 2ms/step - loss: 0.1348 - accuracy: 0.9459\n",
      "Epoch 3/10\n",
      "6250/6250 [==============================] - 15s 2ms/step - loss: 0.1250 - accuracy: 0.9489\n",
      "Epoch 4/10\n",
      "6250/6250 [==============================] - 15s 2ms/step - loss: 0.1149 - accuracy: 0.9540\n",
      "Epoch 5/10\n",
      "6250/6250 [==============================] - 15s 2ms/step - loss: 0.0549 - accuracy: 0.9828\n",
      "Epoch 6/10\n",
      "6250/6250 [==============================] - 15s 2ms/step - loss: 0.0172 - accuracy: 0.9987\n",
      "Epoch 7/10\n",
      "6250/6250 [==============================] - 15s 2ms/step - loss: 0.0111 - accuracy: 0.9996\n",
      "Epoch 8/10\n",
      "6250/6250 [==============================] - 15s 2ms/step - loss: 0.0077 - accuracy: 0.9999\n",
      "Epoch 9/10\n",
      "6250/6250 [==============================] - 15s 2ms/step - loss: 0.0055 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "6250/6250 [==============================] - 15s 2ms/step - loss: 0.0040 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x237ff34fb50>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(xt_train, yt_train, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4db6992",
   "metadata": {},
   "source": [
    "The first line, Epoch 1/10, describes which epoch the model is currently running. For efficiency, the training data set is broken into 'batches'. The default size of a batch in Tensorflow is 32. There are 200000 examples in our expanded data set or 6250 batches. The notation on the 2nd line 6250/6250 [==== is describing which batch has been executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3976357b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1(2, 3):\n",
      " [[-10.302093     0.02421251 -17.065557  ]\n",
      " [ -0.20933619  -8.321201   -14.3162775 ]] \n",
      "b1(3,): [-11.362192 -10.406691  -2.430029]\n",
      "\n",
      "W2(3, 1):\n",
      " [[-42.589046]\n",
      " [-38.053837]\n",
      " [ 30.425957]] \n",
      "b2(1,): [-8.675089]\n"
     ]
    }
   ],
   "source": [
    "# Updated weights\n",
    "w1, b1 = model.get_layer(\"layer1\").get_weights()\n",
    "w2, b2 = model.get_layer(\"layer2\").get_weights()\n",
    "print(f\"W1{w1.shape}:\\n\", w1, f\"\\nb1{b1.shape}:\", b1)\n",
    "print()\n",
    "print(f\"W2{w2.shape}:\\n\", w2, f\"\\nb2{b2.shape}:\", b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a33d08ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 159ms/step\n"
     ]
    }
   ],
   "source": [
    "X_test = np.array([\n",
    "    [200,13.9],  # postive example\n",
    "    [200,17]])   # negative example\n",
    "Xn_test = normal_l(X_test)\n",
    "prediction = model.predict(Xn_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "104b58e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.6740770e-01]\n",
      " [1.6548086e-04]]\n"
     ]
    }
   ],
   "source": [
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b355c801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "predict = (prediction >= 0.5).astype(int)\n",
    "print(predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bbf944",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
